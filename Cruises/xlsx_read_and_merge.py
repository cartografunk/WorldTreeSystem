#Cruises/xlsx_read_and_merge.py
from core.schema import (
    COLUMNS,
)
from core.schema_helpers import rename_columns_using_schema, clean_column_name, get_column as get_column
from core.libs import pd, warnings, Path, os, tqdm, traceback, sleep

from Cruises.utils.metadata_extractor import extract_metadata_from_excel
from Cruises.utils.onedriver import force_download
from Cruises.general_importer import cast_dataframe


warnings.filterwarnings(
    "ignore",
    message="Data Validation extension is not supported and will be removed",
    category=UserWarning,
    module="openpyxl",
)
warnings.filterwarnings(
    "ignore",
    message="Conditional Formatting extension is not supported and will be removed",
    category=UserWarning,
    module="openpyxl",
)
warnings.filterwarnings(
    "ignore",
    message="The behavior of DataFrame concatenation with empty or all-NA entries is deprecated",
    category=FutureWarning,
    module="pandas.core.reshape.concat"
)



def read_metadata_and_input(file_path: str) -> tuple[pd.DataFrame | None, dict]:
    """
    Abre un XLSX, extrae:
     - df_input: la hoja 'Input' o 'DataInput'
     - meta: dict con contract_code, farmer_name, cruise_date
    A la vez aplica:
     1) Limpieza de nombres
     2) casteo de tipos (cast_dataframe)
     3) renombrado global basado en schema (rename_columns_using_schema)
     4) eliminaci√≥n de cat_*_id viejas
     5) verificaci√≥n r√°pida de columnas l√≥gicas de cat√°logo
    """
    try:
        path = Path(file_path)
        max_retries = 3
        delay_base = 8  # segundos

        for attempt in range(1, max_retries + 1):
            success = force_download(path)

            # Validaci√≥n real de archivo descargado
            if success and path.exists() and path.is_file() and path.stat().st_size > 0:
                break

            print(f"üîÅ Retry {attempt}/{max_retries} para {path.name}‚Ä¶")
            sleep(delay_base * attempt)
        else:
            print(f"‚õî No se pudo acceder a: {file_path} tras {max_retries} intentos")
            return None, {}

        # Proceder con la lectura
        xls = pd.ExcelFile(file_path)
        raw_sheets = xls.sheet_names

        preferred_sheets = ["input (2)", "input", "datainput", "Sheet1"]
        target = None
        for pref in preferred_sheets:
            matches = [s for s in raw_sheets if s.lower().strip() == pref]
            if matches:
                target = matches[0]
                break
        if not target:
            target = raw_sheets[0]

        df = pd.read_excel(xls, sheet_name=target, dtype=str, na_filter=False)

        # 0Ô∏è‚É£ Limpieza inicial: uniformizar todos los encabezados en ‚Äúsnake case‚Äù minimal
        df.columns = [clean_column_name(c) for c in df.columns]

        # 1Ô∏è‚É£  Normaliza dtypes (solo una vez)
        df = cast_dataframe(df)

        # 2Ô∏è‚É£  Renombrado global seg√∫n schema.py
        #     Esto convierte columnas ‚Äúdefecto‚Äù‚Üí‚ÄúDefect‚Äù, ‚Äúespecie‚Äù‚Üí‚ÄúSpecies‚Äù, ‚Äúplagas‚Äù‚Üí‚ÄúPests‚Äù, etc.
        df = rename_columns_using_schema(df)
        #print(">>> Columnas tras rename_columns_using_schema:", df.columns.tolist())

        # 3Ô∏è‚É£  Borrar columnas residuales cat_*_id (si quedaron de corridas anteriores)
        for c in [
            "cat_defect_id",
            "cat_pest_id",
            "cat_disease_id",
            "cat_coppiced_id",
            "cat_permanent_plot_id",
        ]:
            if c in df.columns:
                df = df.drop(columns=[c])
                print(f"üî∏ Eliminada columna residual {c!r}")

        #print(">>> Columnas DESPU√âS de quitar cat_*_id viejas:", df.columns.tolist())

        # 5Ô∏è‚É£ Devolver el DataFrame listo para pasar a normalize_catalogs
        meta = extract_metadata_from_excel(file_path) or {}
        meta["sheet_used"] = target  # ‚Üê esto es clave
        return df, meta

    except Exception as e:
        print(f"[ERROR] {file_path}: {e}")
        traceback.print_exc()
        return None, {}

def combine_files(explicit_files=None, base_path=None, filter_func=None):
    """Combina archivos XLSX de inventario forestal.

    Args:
        base_path (str/Path): Ruta base (se ignora si explicit_files est√° presente)
        filter_func (callable, optional): Funci√≥n para filtrar metadatos
        explicit_files (list, optional): Lista expl√≠cita de paths de archivos a procesar

    Returns:
        pd.DataFrame: DataFrame combinado
    """

    df_list = []
    all_files = []

    # Priorizar archivos expl√≠citos si existen
    if explicit_files:
        all_files = [Path(f) for f in explicit_files]

        #print(f"üóÇÔ∏è Procesando {len(all_files)} archivos expl√≠citos")

    else:  # Modo autom√°tico: buscar en directorio
        base_path = Path(base_path)
        print(f"üìÅ Buscando archivos en: {base_path}")

        for root, _, files in os.walk(base_path):
            for f in files:
                if (f.lower().endswith(".xlsx")
                        and not f.startswith("~$")
                        and "combined_inventory" not in f.lower()):
                    all_files.append(Path(root) / f)

        print(f"üîç Encontrados {len(all_files)} archivos XLSX")

    if not all_files:
        print("‚ùå No hay archivos para procesar")
        return pd.DataFrame()

    print("‚öôÔ∏è Iniciando procesamiento de archivos...")
    for path in tqdm(all_files, unit="archivo"):
        # ‚¨áÔ∏è Verifica y descarga si est√° en la nube
        if not force_download(path):
            continue  # Silencioso

        file = path.name
        try:
            # Leer archivo y metadatos
            df, meta = read_metadata_and_input(path)

            if df is None:
                print(f"   ‚ö†Ô∏è  Archivo no procesado: {file}")
                continue

            if df.empty:
                print(f"   ‚ö†Ô∏è  Archivo vac√≠o: {file}")
                continue

            df = rename_columns_using_schema(df)

            # Validaci√≥n b√°sica de columnas
            required_cols = [get_column("tree_number"), get_column("status")]
            missing = [c for c in required_cols if c not in df.columns]
            if missing:
                print(f"   ‚ùå Faltan columnas clave: {', '.join(missing)}")
                continue

            # Filtrado por metadatos
            if filter_func and not filter_func(meta):
                print(f"   üö´ Filtrado por c√≥digos: {file}")
                continue

            # Limpieza inicial
            df = df.dropna(subset=required_cols, how="all")
            df = df.reset_index(drop=True)

            # A√±adir metadatos
            df["contractcode"] = meta.get("contract_code", "DESCONOCIDO")
            df["farmername"] = meta.get("farmer_name", "SIN_NOMBRE")
            df["cruisedate"] = meta.get("cruise_date", pd.NaT)

            df_list.append(df)
            #print(f"   ‚úÖ Procesado exitoso: {len(df)} filas")

        except Exception as e:
            print(f"   üî• Error cr√≠tico en {file}: {str(e)}")
            continue

    if not df_list:
        print("‚ùå Ning√∫n archivo pudo ser procesado")
        return pd.DataFrame()

    combined = pd.concat(df_list, ignore_index=True)
    print("\nüìä Combinaci√≥n finalizada")
    print(f"üå≥ Total de √°rboles procesados: {len(combined):,}")
    #print(f"üìÖ Rango de fechas: {combined['cruisedate'].min()} a {combined['cruisedate'].max()}")

    return combined